{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright 2020 (c) Cognizant Digital Business, Evolutionary AI. All rights reserved. Issued under the Apache 2.0 License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Predictor: SIR Rollout Predictor\n",
    "\n",
    "This example contains basic functionality for training and evaluating a linear predictor that rolls out predictions day-by-day.\n",
    "\n",
    "First, a training data set is created from historical case and npi data.\n",
    "\n",
    "Second, a linear model is trained to predict future cases from prior case data along with prior and future npi data.\n",
    "The model is an off-the-shelf sklearn Lasso model, that uses a positive weight constraint to enforce the assumption that increased npis has a negative correlation with future cases.\n",
    "\n",
    "Third, a sample evaluation set is created, and the predictor is applied to this evaluation set to produce prediction results in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "#from os.path import pardir, sep \n",
    "sys.path.insert(1,'/'+os.path.join(*os.getcwd().split('/')[:-2])+'/utils')\n",
    "from custom_models import SIRRegressor\n",
    "from df_with_temperature import add_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main source for the training data\n",
    "DATA_URL = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "# Local file\n",
    "DATA_FILE = 'data/OxCGRT_latest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/OxCGRT_latest.csv', <http.client.HTTPMessage at 0x7f51bf7c1ba8>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "urllib.request.urlretrieve(DATA_URL, DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data from local file\n",
    "df = pd.read_csv(DATA_FILE, \n",
    "                 parse_dates=['Date'],\n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 dtype={\"RegionName\": str,\n",
    "                        \"RegionCode\": str},\n",
    "                 error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CountryName', 'CountryCode', 'RegionName', 'RegionCode',\n",
       "       'Jurisdiction', 'Date', 'C1_School closing', 'C1_Flag',\n",
       "       'C2_Workplace closing', 'C2_Flag', 'C3_Cancel public events', 'C3_Flag',\n",
       "       'C4_Restrictions on gatherings', 'C4_Flag', 'C5_Close public transport',\n",
       "       'C5_Flag', 'C6_Stay at home requirements', 'C6_Flag',\n",
       "       'C7_Restrictions on internal movement', 'C7_Flag',\n",
       "       'C8_International travel controls', 'E1_Income support', 'E1_Flag',\n",
       "       'E2_Debt/contract relief', 'E3_Fiscal measures',\n",
       "       'E4_International support', 'H1_Public information campaigns',\n",
       "       'H1_Flag', 'H2_Testing policy', 'H3_Contact tracing',\n",
       "       'H4_Emergency investment in healthcare', 'H5_Investment in vaccines',\n",
       "       'H6_Facial Coverings', 'H6_Flag', 'H7_Vaccination policy', 'H7_Flag',\n",
       "       'M1_Wildcard', 'ConfirmedCases', 'ConfirmedDeaths', 'StringencyIndex',\n",
       "       'StringencyIndexForDisplay', 'StringencyLegacyIndex',\n",
       "       'StringencyLegacyIndexForDisplay', 'GovernmentResponseIndex',\n",
       "       'GovernmentResponseIndexForDisplay', 'ContainmentHealthIndex',\n",
       "       'ContainmentHealthIndexForDisplay', 'EconomicSupportIndex',\n",
       "       'EconomicSupportIndexForDisplay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, restrict training data to that before a hypothetical predictor submission date\n",
    "HYPOTHETICAL_SUBMISSION_DATE = np.datetime64(\"2020-07-31\")\n",
    "df = df[df.Date <= HYPOTHETICAL_SUBMISSION_DATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RegionID column that combines CountryName and RegionName for easier manipulation of data\n",
    "df['GeoID'] = df['CountryName'] + '__' + df['RegionName'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cases column\n",
    "df['NewCases'] = df.groupby('GeoID').ConfirmedCases.diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns of interest\n",
    "id_cols = ['CountryName',\n",
    "           'RegionName',\n",
    "           'GeoID',\n",
    "           'Date']\n",
    "# Columns we care just about the last value (usually it's always the same value for most of them)\n",
    "adj_cols_fixed=[ 'PastCases','Population',\n",
    "       'Population Density (# per km2)',\n",
    "       'Urban population (% of total population)',\n",
    "       'Population ages 65 and above (% of total population)',\n",
    "       'GDP per capita (current US$)', 'Obesity Rate (%)', 'Cancer Rate (%)',\n",
    "       'Share of Deaths from Smoking (%)', 'Pneumonia Death Rate (per 100K)',\n",
    "       'Share of Deaths from Air Pollution (%)',\n",
    "       'CO2 emissions (metric tons per capita)',\n",
    "       'Air transport (# carrier departures worldwide)']\n",
    "\n",
    "# Columns we would like to include for the last nb_lookback days\n",
    "adj_cols_time=['TemperatureC']\n",
    "\n",
    "\n",
    "cases_col = ['NewCases']\n",
    "npi_cols = ['C1_School closing',\n",
    "            'C2_Workplace closing',\n",
    "            'C3_Cancel public events',\n",
    "            'C4_Restrictions on gatherings',\n",
    "            'C5_Close public transport',\n",
    "            'C6_Stay at home requirements',\n",
    "            'C7_Restrictions on internal movement',\n",
    "            'C8_International travel controls',\n",
    "            'H1_Public information campaigns',\n",
    "            'H2_Testing policy',\n",
    "            'H3_Contact tracing',\n",
    "            'H6_Facial Coverings']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing case values by interpolation and setting NaNs to 0\n",
    "df.update(df.groupby('GeoID').NewCases.apply(\n",
    "    lambda group: group.interpolate()).fillna(0))\n",
    "df.update(df.groupby('GeoID').ConfirmedCases.apply(\n",
    "    lambda group: group.interpolate()).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing NPIs by assuming they are the same as previous day\n",
    "for npi_col in npi_cols:\n",
    "    df.update(df.groupby('GeoID')[npi_col].ffill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_population_data(df):\n",
    "    \"\"\"\n",
    "    Add additional data like population, Cancer rate, etc..  in Oxford data.\n",
    "    But now it removes rows with at least 1 Nan\n",
    "    \"\"\"\n",
    "    more_df = pd.read_csv(\"data/Additional_Context_Data_Global.csv\")\n",
    "    more_df.dropna(inplace=True)\n",
    "    new_df=more_df.merge(df,how='inner',left_on=['CountryName',\"CountryCode\"],right_on=['CountryName',\"CountryCode\"])\n",
    "    return new_df\n",
    "\n",
    "def add_temp(df):\n",
    "    '''Use this only on the Oxford dataframe.\n",
    "    Return the same dataframe with a column temperature taken from data/country_temperatures.csv'''\n",
    "\n",
    "    df_T=pd.read_csv('data/country_temperatures.csv',parse_dates=['Date'])\n",
    "    df_T=df.merge(df_T,how='left',left_on=['CountryName','Date'],right_on=['CountryName','Date'])\n",
    "    return df_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=add_population_data(df)\n",
    "df=add_temp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PastCases']=df.ConfirmedCases.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CountryName', 'CountryCode', 'Population',\n",
       "       'Population Density (# per km2)',\n",
       "       'Urban population (% of total population)',\n",
       "       'Population ages 65 and above (% of total population)',\n",
       "       'GDP per capita (current US$)', 'Obesity Rate (%)', 'Cancer Rate (%)',\n",
       "       'Share of Deaths from Smoking (%)', 'Pneumonia Death Rate (per 100K)',\n",
       "       'Share of Deaths from Air Pollution (%)',\n",
       "       'CO2 emissions (metric tons per capita)',\n",
       "       'Air transport (# carrier departures worldwide)', 'RegionName',\n",
       "       'RegionCode', 'Jurisdiction', 'Date', 'C1_School closing', 'C1_Flag',\n",
       "       'C2_Workplace closing', 'C2_Flag', 'C3_Cancel public events', 'C3_Flag',\n",
       "       'C4_Restrictions on gatherings', 'C4_Flag', 'C5_Close public transport',\n",
       "       'C5_Flag', 'C6_Stay at home requirements', 'C6_Flag',\n",
       "       'C7_Restrictions on internal movement', 'C7_Flag',\n",
       "       'C8_International travel controls', 'E1_Income support', 'E1_Flag',\n",
       "       'E2_Debt/contract relief', 'E3_Fiscal measures',\n",
       "       'E4_International support', 'H1_Public information campaigns',\n",
       "       'H1_Flag', 'H2_Testing policy', 'H3_Contact tracing',\n",
       "       'H4_Emergency investment in healthcare', 'H5_Investment in vaccines',\n",
       "       'H6_Facial Coverings', 'H6_Flag', 'H7_Vaccination policy', 'H7_Flag',\n",
       "       'M1_Wildcard', 'ConfirmedCases', 'ConfirmedDeaths', 'StringencyIndex',\n",
       "       'StringencyIndexForDisplay', 'StringencyLegacyIndex',\n",
       "       'StringencyLegacyIndexForDisplay', 'GovernmentResponseIndex',\n",
       "       'GovernmentResponseIndexForDisplay', 'ContainmentHealthIndex',\n",
       "       'ContainmentHealthIndexForDisplay', 'EconomicSupportIndex',\n",
       "       'EconomicSupportIndexForDisplay', 'GeoID', 'NewCases', 'TemperatureC',\n",
       "       'PastCases'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "H6_Flag                              20165\n",
       "H7_Vaccination policy                31311\n",
       "H7_Flag                                  0\n",
       "M1_Wildcard                              0\n",
       "ConfirmedCases                       42600\n",
       "ConfirmedDeaths                      36949\n",
       "StringencyIndex                      42386\n",
       "StringencyIndexForDisplay            42386\n",
       "StringencyLegacyIndex                42386\n",
       "StringencyLegacyIndexForDisplay      42386\n",
       "GovernmentResponseIndex              42380\n",
       "GovernmentResponseIndexForDisplay    42380\n",
       "ContainmentHealthIndex               42385\n",
       "ContainmentHealthIndexForDisplay     42385\n",
       "EconomicSupportIndex                 36844\n",
       "EconomicSupportIndexForDisplay       36844\n",
       "GeoID                                42600\n",
       "NewCases                             42600\n",
       "TemperatureC                         42600\n",
       "PastCases                            42600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns of interest\n",
    "df = df[id_cols+ cases_col  +adj_cols_fixed+ adj_cols_time+ npi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountryName                                             42600\n",
       "RegionName                                              16827\n",
       "GeoID                                                   42600\n",
       "Date                                                    42600\n",
       "NewCases                                                42600\n",
       "PastCases                                               42600\n",
       "Population                                              42600\n",
       "Population Density (# per km2)                          42600\n",
       "Urban population (% of total population)                42600\n",
       "Population ages 65 and above (% of total population)    42600\n",
       "GDP per capita (current US$)                            42600\n",
       "Obesity Rate (%)                                        42600\n",
       "Cancer Rate (%)                                         42600\n",
       "Share of Deaths from Smoking (%)                        42600\n",
       "Pneumonia Death Rate (per 100K)                         42600\n",
       "Share of Deaths from Air Pollution (%)                  42600\n",
       "CO2 emissions (metric tons per capita)                  42600\n",
       "Air transport (# carrier departures worldwide)          42600\n",
       "TemperatureC                                            42600\n",
       "C1_School closing                                       42600\n",
       "C2_Workplace closing                                    42600\n",
       "C3_Cancel public events                                 42600\n",
       "C4_Restrictions on gatherings                           42600\n",
       "C5_Close public transport                               42600\n",
       "C6_Stay at home requirements                            42600\n",
       "C7_Restrictions on internal movement                    42600\n",
       "C8_International travel controls                        42600\n",
       "H1_Public information campaigns                         42600\n",
       "H2_Testing policy                                       42600\n",
       "H3_Contact tracing                                      42600\n",
       "H6_Facial Coverings                                     42600\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36400, 433)\n",
      "(36400,)\n"
     ]
    }
   ],
   "source": [
    "lookback_days=30\n",
    "infection_days=15\n",
    "single_pred_days=1\n",
    "def skl_format(df, lookback_days=30):\n",
    "    \"\"\"\n",
    "    Takes data and makes a formatting for sklearn\n",
    "    \"\"\"\n",
    "    # Create training data across all countries for predicting one day ahead\n",
    "    X_cols = cases_col + adj_cols_fixed + adj_cols_time + npi_cols\n",
    "    y_col = cases_col\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "    geo_ids = df.GeoID.unique()\n",
    "    for g in geo_ids:\n",
    "        gdf = df[df.GeoID == g]\n",
    "        all_case_data = np.array(gdf[cases_col])\n",
    "        all_adj_fixed_data=np.array(gdf[adj_cols_fixed])\n",
    "        all_adj_time_data=np.array(gdf[adj_cols_time])\n",
    "        all_npi_data = np.array(gdf[npi_cols])\n",
    "        \n",
    "\n",
    "        # Create one sample for each day where we have enough data\n",
    "        # Each sample consists of cases and npis for previous lookback_days\n",
    "        nb_total_days = len(gdf)\n",
    "        for d in range(lookback_days, nb_total_days - 1):\n",
    "            X_cases = all_case_data[d-lookback_days:d]\n",
    "\n",
    "                       \n",
    "            # \n",
    "            X_adj_fixed=all_adj_fixed_data[d-1]\n",
    "            X_adj_time=all_adj_time_data[d-lookback_days:d]\n",
    "            \n",
    "            # Take negative of npis to support positive\n",
    "            # weight constraint in Lasso.\n",
    "            X_npis = -all_npi_data[d - lookback_days:d]\n",
    "\n",
    "            # Flatten all input data so it fits Lasso input format.\n",
    "            X_sample = np.concatenate([X_cases.flatten(),X_adj_fixed.flatten(),X_adj_time.flatten(),\n",
    "                                       X_npis.flatten()])\n",
    "            y_sample = all_case_data[d]\n",
    "            X_samples.append(X_sample)\n",
    "            y_samples.append(y_sample)\n",
    "\n",
    "    X_samples = np.array(X_samples)\n",
    "    y_samples = np.array(y_samples).flatten()\n",
    "\n",
    "    return X_samples, y_samples\n",
    "\n",
    "X_samples, y_samples= skl_format(df,lookback_days)\n",
    "print(X_samples.shape)\n",
    "print(y_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful function to compute mae\n",
    "def mae(pred, true):\n",
    "    return np.mean(np.abs(pred - true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_samples,\n",
    "                                                    y_samples,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SIRRegressor()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train Lasso model.\n",
    "# Set positive=True to enforce assumption that cases are positively correlated\n",
    "# with future cases and npis are negatively correlated.\n",
    "model = SIRRegressor()\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 443.32663724143475\n",
      "Test MAE: 484.3206704815507\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "train_preds = model.predict(X_train)\n",
    "train_preds = np.maximum(train_preds, 0) # Don't predict negative cases\n",
    "print('Train MAE:', mae(train_preds, y_train))\n",
    "\n",
    "test_preds = model.predict(X_test)\n",
    "test_preds = np.maximum(test_preds, 0) # Don't predict negative cases\n",
    "print('Test MAE:', mae(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day -7 NewCases 0.001065730099620814\n",
      "Day -6 NewCases 0.43959907319825564\n",
      "Day -5 NewCases 0.2169863608901526\n",
      "Day -4 NewCases 0.059170149304116265\n",
      "Day -3 NewCases 0.06927720478040666\n",
      "Day -2 NewCases 0.052120436323877355\n",
      "Day -1 NewCases 0.23833173044505512\n",
      "Day -26 C6_Stay at home requirements 4.428407638537636\n",
      "Day -22 C2_Workplace closing 9.735422362636706\n",
      "Day -17 C2_Workplace closing 5.512183404210009\n",
      "Intercept 26.368862164828613\n"
     ]
    }
   ],
   "source": [
    "# Inspect the learned feature coefficients for the model\n",
    "# to see what features it's paying attention to.\n",
    "\n",
    "# Give names to the features\n",
    "x_col_names = []\n",
    "for d in range(-nb_lookback_days, 0):\n",
    "    x_col_names.append('Day ' + str(d) + ' ' + cases_col[0])\n",
    "for d in range(-nb_lookback_days, 1):\n",
    "    for col_name in npi_cols:\n",
    "        x_col_names.append('Day ' + str(d) + ' ' + col_name)\n",
    "\n",
    "# View non-zero coefficients\n",
    "for (col, coeff) in zip(x_col_names, list(model.coef_)):\n",
    "    if coeff != 0.:\n",
    "        print(col, coeff)\n",
    "print('Intercept', model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to file\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "with open('models/model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that the predictor has been trained and saved, this section contains the functionality for evaluating it on sample evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module to get the latest changes\n",
    "import predict\n",
    "from importlib import reload\n",
    "reload(predict)\n",
    "from predict import predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds_df = predict_df(\"2020-08-01\", \"2020-08-31\", path_to_ips_file=\"../../../validation/data/2020-09-30_historical_ip.csv\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the predictions\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "This is how the predictor is going to be called during the competition.  \n",
    "!!! PLEASE DO NOT CHANGE THE API !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py -s 2020-08-01 -e 2020-08-04 -ip ../../../validation/data/2020-09-30_historical_ip.csv -o predictions/2020-08-01_2020-08-04.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head predictions/2020-08-01_2020-08-04.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test cases\n",
    "We can generate a prediction file. Let's validate a few cases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from covid_xprize.validation.predictor_validation import validate_submission\n",
    "\n",
    "def validate(start_date, end_date, ip_file, output_file):\n",
    "    # First, delete any potential old file\n",
    "    try:\n",
    "        os.remove(output_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    # Then generate the prediction, calling the official API\n",
    "    !python predict.py -s {start_date} -e {end_date} -ip {ip_file} -o {output_file}\n",
    "    \n",
    "    # And validate it\n",
    "    errors = validate_submission(start_date, end_date, ip_file, output_file)\n",
    "    if errors:\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 days, no gap\n",
    "- All countries and regions\n",
    "- Official number of cases is known up to start_date\n",
    "- Intervention Plans are the official ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(start_date=\"2020-08-01\",\n",
    "         end_date=\"2020-08-04\",\n",
    "         ip_file=\"../../../validation/data/2020-09-30_historical_ip.csv\",\n",
    "         output_file=\"predictions/val_4_days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 month in the future\n",
    "- 2 countries only\n",
    "- there's a gap between date of last known number of cases and start_date\n",
    "- For future dates, Intervention Plans contains scenarios for which predictions are requested to answer the question: what will happen if we apply these plans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "validate(start_date=\"2021-01-01\",\n",
    "         end_date=\"2021-01-31\",\n",
    "         ip_file=\"../../../validation/data/future_ip.csv\",\n",
    "         output_file=\"predictions/val_1_month_future.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 180 days, from a future date, all countries and regions\n",
    "- Prediction start date is 1 week from now. (i.e. assuming submission date is 1 week from now)  \n",
    "- Prediction end date is 6 months after start date.  \n",
    "- Prediction is requested for all available countries and regions.  \n",
    "- Intervention plan scenario: freeze last known intervention plans for each country and region.  \n",
    "\n",
    "As the number of cases is not known yet between today and start date, but the model relies on them, the model has to predict them in order to use them.  \n",
    "This test is the most demanding test. It should take less than 1 hour to generate the prediction file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime.now() + timedelta(days=7)\n",
    "start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "end_date = start_date + timedelta(days=180)\n",
    "end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "print(f\"Start date: {start_date_str}\")\n",
    "print(f\"End date: {end_date_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from covid_xprize.validation.scenario_generator import get_raw_data, generate_scenario, NPI_COLUMNS\n",
    "DATA_FILE = 'data/OxCGRT_latest.csv'\n",
    "latest_df = get_raw_data(DATA_FILE, latest=True)\n",
    "scenario_df = generate_scenario(start_date_str, end_date_str, latest_df, countries=None, scenario=\"Freeze\")\n",
    "scenario_file = \"predictions/180_days_future_scenario.csv\"\n",
    "scenario_df.to_csv(scenario_file, index=False)\n",
    "print(f\"Saved scenario to {scenario_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "validate(start_date=start_date_str,\n",
    "         end_date=end_date_str,\n",
    "         ip_file=scenario_file,\n",
    "         output_file=\"predictions/val_6_month_future.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xprize",
   "language": "python",
   "name": "xprize"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "# Copyright 2020 (c) Cognizant Digital Business, Evolutionary AI. All rights reserved. Issued under the Apache 2.0 License."
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
