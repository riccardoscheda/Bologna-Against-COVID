{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Predictor: XGBoost Predictor\n",
    "\n",
    "This example contains basic functionality for training and evaluating a XGBoost predictor.\n",
    "\n",
    "First, a training data set is created from historical case and npi data.\n",
    "\n",
    "Second, a XGBoost model is trained to predict future cases from prior case data along with prior and future npi data.\n",
    "The model is an off-the-shelf sklearn XGBoost model.\n",
    "\n",
    "Third, a sample evaluation set is created, and the predictor is applied to this evaluation set to produce prediction results in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darren.sargent/venv/covid-xprize/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (2,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load historical data from URL\n",
    "URL = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "df = pd.read_csv(URL, \n",
    "                 parse_dates=['Date'],\n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, restrict training data to that before a hypothetical predictor submission date\n",
    "HYPOTHETICAL_SUBMISSION_DATE = np.datetime64(\"2020-07-31\")\n",
    "df = df[df.Date <= HYPOTHETICAL_SUBMISSION_DATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RegionID column that combines CountryName and RegionName for easier manipulation of data\n",
    "df['GeoID'] = df['CountryName'] + '__' + df['RegionName'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cases column\n",
    "df['NewCases'] = df.groupby('GeoID').ConfirmedCases.diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns of interest\n",
    "id_cols = ['CountryName',\n",
    "           'RegionName',\n",
    "           'GeoID',\n",
    "           'Date']\n",
    "cases_col = ['NewCases']\n",
    "npi_cols = ['C1_School closing',\n",
    "            'C2_Workplace closing',\n",
    "            'C3_Cancel public events',\n",
    "            'C4_Restrictions on gatherings',\n",
    "            'C5_Close public transport',\n",
    "            'C6_Stay at home requirements',\n",
    "            'C7_Restrictions on internal movement',\n",
    "            'C8_International travel controls',\n",
    "            'H1_Public information campaigns',\n",
    "            'H2_Testing policy',\n",
    "            'H3_Contact tracing']\n",
    "df = df[id_cols + cases_col + npi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing case values by interpolation and setting NaNs to 0\n",
    "df.update(df.groupby('GeoID').NewCases.apply(\n",
    "    lambda group: group.interpolate()).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing NPIs by assuming they are the same as previous day\n",
    "for npi_col in npi_cols:\n",
    "    df.update(df.groupby('GeoID')[npi_col].ffill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryName</th>\n",
       "      <th>RegionName</th>\n",
       "      <th>GeoID</th>\n",
       "      <th>Date</th>\n",
       "      <th>NewCases</th>\n",
       "      <th>C1_School closing</th>\n",
       "      <th>C2_Workplace closing</th>\n",
       "      <th>C3_Cancel public events</th>\n",
       "      <th>C4_Restrictions on gatherings</th>\n",
       "      <th>C5_Close public transport</th>\n",
       "      <th>C6_Stay at home requirements</th>\n",
       "      <th>C7_Restrictions on internal movement</th>\n",
       "      <th>C8_International travel controls</th>\n",
       "      <th>H1_Public information campaigns</th>\n",
       "      <th>H2_Testing policy</th>\n",
       "      <th>H3_Contact tracing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aruba__nan</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aruba__nan</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aruba__nan</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aruba__nan</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aruba__nan</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64048</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe__nan</td>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>78.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64049</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe__nan</td>\n",
       "      <td>2020-07-28</td>\n",
       "      <td>192.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64050</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe__nan</td>\n",
       "      <td>2020-07-29</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64051</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe__nan</td>\n",
       "      <td>2020-07-30</td>\n",
       "      <td>62.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64052</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zimbabwe__nan</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>213.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51333 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CountryName RegionName          GeoID       Date  NewCases  \\\n",
       "0           Aruba        NaN     Aruba__nan 2020-01-01       0.0   \n",
       "1           Aruba        NaN     Aruba__nan 2020-01-02       0.0   \n",
       "2           Aruba        NaN     Aruba__nan 2020-01-03       0.0   \n",
       "3           Aruba        NaN     Aruba__nan 2020-01-04       0.0   \n",
       "4           Aruba        NaN     Aruba__nan 2020-01-05       0.0   \n",
       "...           ...        ...            ...        ...       ...   \n",
       "64048    Zimbabwe        NaN  Zimbabwe__nan 2020-07-27      78.0   \n",
       "64049    Zimbabwe        NaN  Zimbabwe__nan 2020-07-28     192.0   \n",
       "64050    Zimbabwe        NaN  Zimbabwe__nan 2020-07-29     113.0   \n",
       "64051    Zimbabwe        NaN  Zimbabwe__nan 2020-07-30      62.0   \n",
       "64052    Zimbabwe        NaN  Zimbabwe__nan 2020-07-31     213.0   \n",
       "\n",
       "       C1_School closing  C2_Workplace closing  C3_Cancel public events  \\\n",
       "0                    0.0                   0.0                      0.0   \n",
       "1                    0.0                   0.0                      0.0   \n",
       "2                    0.0                   0.0                      0.0   \n",
       "3                    0.0                   0.0                      0.0   \n",
       "4                    0.0                   0.0                      0.0   \n",
       "...                  ...                   ...                      ...   \n",
       "64048                3.0                   1.0                      2.0   \n",
       "64049                3.0                   1.0                      2.0   \n",
       "64050                3.0                   1.0                      2.0   \n",
       "64051                3.0                   1.0                      2.0   \n",
       "64052                3.0                   1.0                      2.0   \n",
       "\n",
       "       C4_Restrictions on gatherings  C5_Close public transport  \\\n",
       "0                                0.0                        0.0   \n",
       "1                                0.0                        0.0   \n",
       "2                                0.0                        0.0   \n",
       "3                                0.0                        0.0   \n",
       "4                                0.0                        0.0   \n",
       "...                              ...                        ...   \n",
       "64048                            3.0                        1.0   \n",
       "64049                            3.0                        1.0   \n",
       "64050                            3.0                        1.0   \n",
       "64051                            3.0                        1.0   \n",
       "64052                            3.0                        1.0   \n",
       "\n",
       "       C6_Stay at home requirements  C7_Restrictions on internal movement  \\\n",
       "0                               0.0                                   0.0   \n",
       "1                               0.0                                   0.0   \n",
       "2                               0.0                                   0.0   \n",
       "3                               0.0                                   0.0   \n",
       "4                               0.0                                   0.0   \n",
       "...                             ...                                   ...   \n",
       "64048                           2.0                                   2.0   \n",
       "64049                           2.0                                   2.0   \n",
       "64050                           2.0                                   2.0   \n",
       "64051                           2.0                                   2.0   \n",
       "64052                           2.0                                   2.0   \n",
       "\n",
       "       C8_International travel controls  H1_Public information campaigns  \\\n",
       "0                                   0.0                              0.0   \n",
       "1                                   0.0                              0.0   \n",
       "2                                   0.0                              0.0   \n",
       "3                                   0.0                              0.0   \n",
       "4                                   0.0                              0.0   \n",
       "...                                 ...                              ...   \n",
       "64048                               4.0                              2.0   \n",
       "64049                               4.0                              2.0   \n",
       "64050                               4.0                              2.0   \n",
       "64051                               4.0                              2.0   \n",
       "64052                               4.0                              2.0   \n",
       "\n",
       "       H2_Testing policy  H3_Contact tracing  \n",
       "0                    0.0                 0.0  \n",
       "1                    0.0                 0.0  \n",
       "2                    0.0                 0.0  \n",
       "3                    0.0                 0.0  \n",
       "4                    0.0                 0.0  \n",
       "...                  ...                 ...  \n",
       "64048                1.0                 1.0  \n",
       "64049                1.0                 1.0  \n",
       "64050                1.0                 1.0  \n",
       "64051                1.0                 1.0  \n",
       "64052                1.0                 1.0  \n",
       "\n",
       "[51333 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next 2 cells: Functions for training one model for each day into the future we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data across all countries for predicting nb_days_ahead\n",
    "def create_training_data(nb_days_ahead, nb_lookback_days, df, cases_col, npi_cols):\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "    geo_ids = df.GeoID.unique()\n",
    "    for g in geo_ids:\n",
    "        gdf = df[df.GeoID == g]\n",
    "        all_case_data = np.array(gdf[cases_col])\n",
    "        all_npi_data = np.array(gdf[npi_cols])\n",
    "\n",
    "        # Create one sample for each day where we have enough data\n",
    "        nb_total_days = len(gdf)\n",
    "        for day  in range(nb_lookback_days, nb_total_days - nb_days_ahead):\n",
    "            X_cases = all_case_data[day - nb_lookback_days:day ]\n",
    "\n",
    "            X_npis = all_npi_data[day  - nb_lookback_days:day  + nb_days_ahead]\n",
    "\n",
    "            # Flatten all input data so it fits XGBoost input format.\n",
    "            X_sample = np.concatenate([X_cases.flatten(),\n",
    "                                       X_npis.flatten()])\n",
    "            y_sample = all_case_data[day  + nb_days_ahead]\n",
    "            X_samples.append(X_sample)\n",
    "            y_samples.append(y_sample)\n",
    "\n",
    "    X_samples = np.array(X_samples)\n",
    "    y_samples = np.array(y_samples).flatten()\n",
    "    return X_samples, y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mae\n",
    "def mae(pred, true):\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "# Use grid search to optimize hyperparameters for XGBoost\n",
    "# Typical result (used in create_and_train_xgb_model):\n",
    "#\n",
    "# {'colsample_bytree': 0.3, 'gamma': 5, 'learning_rate': 0.01, 'max_depth': 10, 'n_estimators': 1000, \n",
    "#  'reg_alpha': 50}\n",
    "#\n",
    "def search_hyperparams(X_samples, y_samples):\n",
    "        # Split data into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_samples,\n",
    "                                                            y_samples,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=301)\n",
    "\n",
    "        tuned_parameters = {\n",
    "            'learning_rate': [0.1, 0.01, 0.001],\n",
    "            'colsample_bytree': [0.3, 0.5, 0.8],\n",
    "            'n_estimators': [1, 10, 100, 1000],\n",
    "            'reg_alpha': [40, 50, 100],\n",
    "            'max_depth': [3, 5, 10],\n",
    "            'gamma': [0, 1, 5]\n",
    "        }\n",
    "\n",
    "        scores = ['neg_mean_absolute_error']\n",
    "\n",
    "        for score in scores:\n",
    "            print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "            print()\n",
    "\n",
    "            clf = GridSearchCV(\n",
    "                 xgb.XGBRegressor(), tuned_parameters, scoring=score, verbose=2\n",
    "            )\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            print(\"Best parameters set found on development set:\")\n",
    "            print()\n",
    "            print(clf.best_params_)\n",
    "            print()\n",
    "            print(\"Grid scores on development set:\")\n",
    "            print()\n",
    "            means = clf.cv_results_['mean_test_score']\n",
    "            stds = clf.cv_results_['std_test_score']\n",
    "            for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "                print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                      % (mean, std * 2, params))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train XGBoost model\n",
    "def create_and_train_xgb_model(X_samples, y_samples):\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_samples,\n",
    "                                                        y_samples,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=301)\n",
    "    \n",
    "    # with future cases and npis are negatively correlated.\n",
    "    model = xgb.XGBRegressor(\n",
    "        colsample_bytree=0.3, \n",
    "        gamma=5,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=10,\n",
    "        n_estimators=1000,\n",
    "        reg_alpha=50\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_preds = model.predict(X_train)\n",
    "    train_preds = np.maximum(train_preds, 0) # Don't predict negative cases\n",
    "    print('Train MAE:', mae(train_preds, y_train))\n",
    "\n",
    "    test_preds = model.predict(X_test)\n",
    "    test_preds = np.maximum(test_preds, 0) # Don't predict negative cases\n",
    "    print('Test MAE:', mae(test_preds, y_test))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days ahead predicting: 0\n"
     ]
    }
   ],
   "source": [
    "# Train a model for each day ahead we want to predict\n",
    "\n",
    "# Set number of past days to use to make predictions\n",
    "nb_lookback_days = 30\n",
    "\n",
    "# Maximum number of days ahead we want to predict\n",
    "max_days_ahead = 31\n",
    "\n",
    "models = {}\n",
    "for nb_days_ahead in range(max_days_ahead):\n",
    "    print('Days ahead predicting:', nb_days_ahead)\n",
    "    X_samples, y_samples = create_training_data(nb_days_ahead, nb_lookback_days, df, cases_col, npi_cols)\n",
    "    X_samples\n",
    "    y_samples\n",
    "    # model = create_and_train_xgb_model(X_samples, y_samples)\n",
    "    models[nb_days_ahead] = create_and_train_xgb_model(X_samples, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Inspect the learned feature coefficients for each model\n",
    "# to see what features they're paying attention to.\n",
    "for nb_days_ahead in range(max_days_ahead):\n",
    "    print('Model for predicting {} days ahead'.format(nb_days_ahead))\n",
    "\n",
    "    model = models[nb_days_ahead]\n",
    "    plot_importance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save models to file\n",
    "with open('models.pkl', 'wb') as models_file:\n",
    "    pickle.dump(models, models_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that the predictor has been trained and saved, this section contains the functionality for evaluating it on sample evaluation data.\n",
    "\n",
    "First, a sample evaluation data set is created of the form that is given to the predictor.\n",
    "\n",
    "Second, the predictor is evaluated on this data set, and a resulting predictions file is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### Create sample evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hypothetical evaluation data\n",
    "nb_eval_days = 31\n",
    "test_df = pd.read_csv(URL, \n",
    "                      parse_dates=['Date'],\n",
    "                      encoding=\"ISO-8859-1\",\n",
    "                      error_bad_lines=False)\n",
    "\n",
    "# Pull out relevant evaluation days\n",
    "test_df = test_df[(test_df.Date > HYPOTHETICAL_SUBMISSION_DATE) & \\\n",
    "                  (test_df.Date <= HYPOTHETICAL_SUBMISSION_DATE + nb_eval_days)]\n",
    "\n",
    "# Only include columns we would see during evaluation\n",
    "test_df = test_df[['CountryName', 'RegionName', 'Date'] + npi_cols]\n",
    "\n",
    "# Fill any missing NPIs by assuming they are the same as previous day\n",
    "for npi_col in npi_cols:\n",
    "    test_df.update(test_df.groupby(['CountryName', 'RegionName'])[npi_col].ffill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df is now in the form of input to a predictor during evaluation\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def predict(start_date: str, end_date: str, path_to_ips_file: str):\n",
    "    \"\"\"\n",
    "    Generates a file with daily new cases predictions for the given countries, regions and npis, between\n",
    "    start_date and end_date, included.\n",
    "    :param start_date: day from which to start making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param end_date: day on which to stop making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param path_to_ips_file: path to a csv file containing the intervention plans between start_date and end_date\n",
    "    :return: Nothing. Saves a csv file called 'start_date_end_date.csv'\n",
    "    with columns \"CountryName,RegionName,Date,PredictedDailyNewCases\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add RegionID column that combines CountryName and RegionName for easier manipulation of data\\n\",\n",
    "    test_df['GeoID'] = test_df['CountryName'] + '__' + test_df['RegionName'].astype(str)\n",
    "\n",
    "    # Copy the test data frame\n",
    "    pred_df = test_df[id_cols].copy()\n",
    "    # Keep only the requested prediction period.\n",
    "    # Note: this period *might* be in the future, and pred_df doesn't necessarily contain the requested rows\n",
    "    pred_df = pred_df[(pred_df.Date >= start_date) & (pred_df.Date <= end_date)]\n",
    "\n",
    "    # Load historical data to use in making predictions in the same way \n",
    "    # df is loaded above to make training data (just copy the reference here for simplicity)\n",
    "    hist_df = df\n",
    "    \n",
    "        # Load models\n",
    "    with open('models.pkl', 'rb') as models_file:\n",
    "        models = pickle.load(models_file)\n",
    "        \n",
    "    # Make predictions for each country,region pair\n",
    "\n",
    "    geo_pred_dfs = []\n",
    "    for g in test_df.GeoID.unique():\n",
    "        print('\\nPredicting for', g)\n",
    "\n",
    "        # Pull out all relevant data for country c\n",
    "        hist_gdf = hist_df[hist_df.GeoID == g]\n",
    "        test_gdf = test_df[test_df.GeoID == g]\n",
    "        X_cases = np.array(hist_gdf[cases_col])[-nb_lookback_days:]\n",
    "        X_hist_npis = np.array(hist_gdf[npi_cols])[-nb_lookback_days:]\n",
    "        future_npi_data = np.array(test_gdf[npi_cols])\n",
    "\n",
    "        # Make prediction for each day\n",
    "        geo_preds = []\n",
    "        nb_days_to_predict = len(future_npi_data)\n",
    "        for days_ahead in range(nb_days_to_predict):\n",
    "\n",
    "            # Prepare data\n",
    "            X_future_npis = future_npi_data[:days_ahead]\n",
    "            X_npis = np.concatenate([X_hist_npis, X_future_npis])\n",
    "            X = np.concatenate([X_cases.flatten(),\n",
    "                                X_npis.flatten()])\n",
    "\n",
    "            # Grab the right model\n",
    "            model = models[days_ahead]\n",
    "\n",
    "            # Make the prediction (reshape so that sklearn is happy)\n",
    "            pred = model.predict(X.reshape(1, -1))[0]\n",
    "            pred = max(0, pred)\n",
    "            geo_preds.append(pred)\n",
    "\n",
    "        # Create geo_pred_df with pred column\n",
    "        geo_pred_df = test_gdf[id_cols].copy()\n",
    "        geo_pred_df['PredictedDailyNewCases'] = geo_preds\n",
    "        geo_pred_dfs.append(geo_pred_df)\n",
    "\n",
    "    # Combine all predictions into a single dataframe\n",
    "    pred_df = pd.concat(geo_pred_dfs)\n",
    "    \n",
    "    # Drop GeoID column to match expected output format\n",
    "    pred_df = pred_df.drop(columns=['GeoID'])\n",
    "    pred_df\n",
    "    \n",
    "    # Write predictions to csv\n",
    "    # Save to expected file name\n",
    "    output_file_name = start_date + \"_\" + end_date + \".csv\"\n",
    "    pred_df.to_csv(output_file_name, index=None)\n",
    "    print(f\"Predictions saved to {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2020-08-01\"\n",
    "end_date = \"2020-08-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(start_date, end_date, path_to_ips_file=\"../2020-08-01_2020-08-31_npis_example.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Check that predictions are written correctly\n",
    "!head 2020-08-01_2020-08-31.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "from validation.validation import validate_submission\n",
    "\n",
    "errors = validate_submission(start_date, end_date, \"2020-08-01_2020-08-31.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\"#%%\n",
     "\"\n",
     "# Example of training an XGBoost model\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
