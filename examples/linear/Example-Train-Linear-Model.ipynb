{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of training a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Predictor: Linear Predictor\n",
    "\n",
    "This example contains basic functionality for training and evaluating a linear predictor.\n",
    "\n",
    "First, a training data set is created from historical case and npi data.\n",
    "\n",
    "Second, a linear model is trained to predict future cases from prior case data along with prior and future npi data.\n",
    "The model is an off-the-shelf sklearn Lasso model, that uses a positive weight constraint to enforce the assumption that increased npis has a negative correlation with future cases. An independent model is trained for each number of days into the future we need to predict.\n",
    "\n",
    "Third, a sample evaluation set is created, and the predictor is applied to this evaluation set to produce prediction results in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main source for the training data\n",
    "DATA_URL = 'https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv'\n",
    "# Local file\n",
    "DATA_FILE = 'data/OxCGRT_latest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "urllib.request.urlretrieve(DATA_URL, DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data from local file\n",
    "df = pd.read_csv(DATA_FILE, \n",
    "                 parse_dates=['Date'],\n",
    "                 encoding=\"ISO-8859-1\",\n",
    "                 error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing, restrict training data to that before a hypothetical predictor submission date\n",
    "HYPOTHETICAL_SUBMISSION_DATE = np.datetime64(\"2020-07-31\")\n",
    "df = df[df.Date <= HYPOTHETICAL_SUBMISSION_DATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add RegionID column that combines CountryName and RegionName for easier manipulation of data\n",
    "df['GeoID'] = df['CountryName'] + '__' + df['RegionName'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new cases column\n",
    "df['NewCases'] = df.groupby('GeoID').ConfirmedCases.diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns of interest\n",
    "id_cols = ['CountryName',\n",
    "           'RegionName',\n",
    "           'GeoID',\n",
    "           'Date']\n",
    "cases_col = ['NewCases']\n",
    "npi_cols = ['C1_School closing',\n",
    "            'C2_Workplace closing',\n",
    "            'C3_Cancel public events',\n",
    "            'C4_Restrictions on gatherings',\n",
    "            'C5_Close public transport',\n",
    "            'C6_Stay at home requirements',\n",
    "            'C7_Restrictions on internal movement',\n",
    "            'C8_International travel controls',\n",
    "            'H1_Public information campaigns',\n",
    "            'H2_Testing policy',\n",
    "            'H3_Contact tracing']\n",
    "df = df[id_cols + cases_col + npi_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing case values by interpolation and setting NaNs to 0\n",
    "df.update(df.groupby('GeoID').NewCases.apply(\n",
    "    lambda group: group.interpolate()).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any missing NPIs by assuming they are the same as previous day\n",
    "for npi_col in npi_cols:\n",
    "    df.update(df.groupby('GeoID')[npi_col].ffill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next 2 cells: Functions for training one model for each day into the future we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data across all countries for predicting nb_days_ahead\n",
    "def create_training_data(nb_days_ahead, nb_lookback_days, df, cases_col, npi_cols):\n",
    "    X_cols = cases_col + npi_cols\n",
    "    y_col = cases_col\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "    geo_ids = df.GeoID.unique()\n",
    "    for g in geo_ids:\n",
    "        gdf = df[df.GeoID == g]\n",
    "        all_case_data = np.array(gdf[cases_col])\n",
    "        all_npi_data = np.array(gdf[npi_cols])\n",
    "\n",
    "        # Create one sample for each day where we have enough data\n",
    "        nb_total_days = len(gdf)\n",
    "        for d in range(nb_lookback_days, nb_total_days - nb_days_ahead):\n",
    "            X_cases = all_case_data[d-nb_lookback_days:d]\n",
    "\n",
    "            # Take negative of npis to support positive\n",
    "            # weight constraint in Lasso.\n",
    "            X_npis = -all_npi_data[d - nb_lookback_days:d + nb_days_ahead]\n",
    "\n",
    "            # Flatten all input data so it fits Lasso input format.\n",
    "            X_sample = np.concatenate([X_cases.flatten(),\n",
    "                                       X_npis.flatten()])\n",
    "            y_sample = all_case_data[d + nb_days_ahead]\n",
    "            X_samples.append(X_sample)\n",
    "            y_samples.append(y_sample)\n",
    "\n",
    "    X_samples = np.array(X_samples)\n",
    "    y_samples = np.array(y_samples).flatten()\n",
    "    return X_samples, y_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mae\n",
    "def mae(pred, true):\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "# Create and train Lasso model\n",
    "def create_and_train_lasso_model(X_samples, y_samples):\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_samples,\n",
    "                                                        y_samples,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=301)\n",
    "    \n",
    "    # Train Lasso model.\n",
    "    # Set positive=True to enforce assumption that cases are positively correlated\n",
    "    # with future cases and npis are negatively correlated.\n",
    "    model = Lasso(alpha=0.1,\n",
    "                  precompute=True,\n",
    "                  max_iter=10000,\n",
    "                  positive=True,\n",
    "                  selection='random')\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_preds = model.predict(X_train)\n",
    "    train_preds = np.maximum(train_preds, 0) # Don't predict negative cases\n",
    "    print('Train MAE:', mae(train_preds, y_train))\n",
    "\n",
    "    test_preds = model.predict(X_test)\n",
    "    test_preds = np.maximum(test_preds, 0) # Don't predict negative cases\n",
    "    print('Test MAE:', mae(test_preds, y_test))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train a model for each day ahead we want to predict\n",
    "\n",
    "# Set number of past days to use to make predictions\n",
    "nb_lookback_days = 30\n",
    "\n",
    "# Maximum number of days ahead we want to predict\n",
    "max_days_ahead = 31\n",
    "\n",
    "models = {}\n",
    "for nb_days_ahead in range(max_days_ahead):\n",
    "    print('Days ahead predicting:', nb_days_ahead)\n",
    "    X_samples, y_samples = create_training_data(nb_days_ahead, nb_lookback_days, df, cases_col, npi_cols)\n",
    "    model = create_and_train_lasso_model(X_samples, y_samples)\n",
    "    models[nb_days_ahead] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the learned feature coefficients for each model\n",
    "# to see what features they're paying attention to.\n",
    "for nb_days_ahead in range(max_days_ahead):\n",
    "    print('Model for predicting {} days ahead'.format(nb_days_ahead))\n",
    "\n",
    "    # Name features\n",
    "    x_col_names = []\n",
    "    for d in range(-nb_lookback_days, 0):\n",
    "        x_col_names.append('Day ' + str(d) + ' ' + cases_col[0])\n",
    "    for d in range(-nb_lookback_days, nb_days_ahead):\n",
    "        for col_name in npi_cols:\n",
    "            x_col_names.append('Day ' + str(d) + ' ' + col_name)\n",
    "\n",
    "    # View non-zero coefficients\n",
    "    model = models[nb_days_ahead]\n",
    "    for (col, coeff) in zip(x_col_names, list(model.coef_)):\n",
    "        if coeff != 0.:\n",
    "            print(col, coeff)\n",
    "    print('Intercept', model.intercept_)\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to file\n",
    "with open('models.pkl', 'wb') as models_file:\n",
    "    pickle.dump(models, models_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that the predictor has been trained and saved, this section contains the functionality for evaluating it on sample evaluation data.\n",
    "\n",
    "First, a sample evaluation data set is created of the form that is given to the predictor.\n",
    "\n",
    "Second, the predictor is evaluated on this data set, and a resulting predictions file is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hypothetical evaluation data\n",
    "nb_eval_days = 31\n",
    "test_df = pd.read_csv(URL, \n",
    "                      parse_dates=['Date'],\n",
    "                      encoding=\"ISO-8859-1\",\n",
    "                      error_bad_lines=False)\n",
    "\n",
    "# Pull out relevant evaluation days\n",
    "test_df = test_df[(test_df.Date > HYPOTHETICAL_SUBMISSION_DATE) & \\\n",
    "                  (test_df.Date <= HYPOTHETICAL_SUBMISSION_DATE + nb_eval_days)]\n",
    "\n",
    "# Only include columns we would see during evaluation\n",
    "test_df = test_df[['CountryName', 'RegionName', 'Date'] + npi_cols]\n",
    "\n",
    "# Fill any missing NPIs by assuming they are the same as previous day\n",
    "for npi_col in npi_cols:\n",
    "    test_df.update(test_df.groupby(['CountryName', 'RegionName'])[npi_col].ffill().fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df is now in the form of input to a predictor during evaluation\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply predictor to the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(start_date: str, end_date: str, path_to_ips_file: str):\n",
    "    \"\"\"\n",
    "    Generates a file with daily new cases predictions for the given countries, regions and npis, between\n",
    "    start_date and end_date, included.\n",
    "    :param start_date: day from which to start making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param end_date: day on which to stop making predictions, as a string, format YYYY-MM-DDD\n",
    "    :param path_to_ips_file: path to a csv file containing the intervention plans between start_date and end_date\n",
    "    :return: Nothing. Saves a csv file called 'start_date_end_date.csv'\n",
    "    with columns \"CountryName,RegionName,Date,PredictedDailyNewCases\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add RegionID column that combines CountryName and RegionName for easier manipulation of data\\n\",\n",
    "    test_df['GeoID'] = test_df['CountryName'] + '__' + test_df['RegionName'].astype(str)\n",
    "\n",
    "    # Copy the test data frame\n",
    "    pred_df = test_df[id_cols].copy()\n",
    "    # Keep only the requested prediction period.\n",
    "    # Note: this period *might* be in the future, and pred_df doesn't necessarily contain the requested rows\n",
    "    pred_df = pred_df[(pred_df.Date >= start_date) & (pred_df.Date <= end_date)]\n",
    "\n",
    "    # Load historical data to use in making predictions in the same way \n",
    "    # df is loaded above to make training data (just copy the reference here for simplicity)\n",
    "    hist_df = df\n",
    "    \n",
    "        # Load models\n",
    "    with open('models.pkl', 'rb') as models_file:\n",
    "        models = pickle.load(models_file)\n",
    "        \n",
    "    # Make predictions for each country,region pair\n",
    "\n",
    "    geo_pred_dfs = []\n",
    "    for g in test_df.GeoID.unique():\n",
    "        print('\\nPredicting for', g)\n",
    "\n",
    "        # Pull out all relevant data for country c\n",
    "        hist_gdf = hist_df[hist_df.GeoID == g]\n",
    "        test_gdf = test_df[test_df.GeoID == g]\n",
    "        X_cases = np.array(hist_gdf[cases_col])[-nb_lookback_days:]\n",
    "        X_hist_npis = np.array(hist_gdf[npi_cols])[-nb_lookback_days:]\n",
    "        future_npi_data = np.array(test_gdf[npi_cols])\n",
    "\n",
    "        # Make prediction for each day\n",
    "        geo_preds = []\n",
    "        nb_days_to_predict = len(future_npi_data)\n",
    "        for days_ahead in range(nb_days_to_predict):\n",
    "\n",
    "            # Prepare data\n",
    "            X_future_npis = future_npi_data[:days_ahead]\n",
    "            X_npis = np.concatenate([X_hist_npis, X_future_npis])\n",
    "            X = np.concatenate([X_cases.flatten(),\n",
    "                                X_npis.flatten()])\n",
    "\n",
    "            # Grab the right model\n",
    "            model = models[days_ahead]\n",
    "\n",
    "            # Make the prediction (reshape so that sklearn is happy)\n",
    "            pred = model.predict(X.reshape(1, -1))[0]\n",
    "            pred = max(0, pred)\n",
    "            geo_preds.append(pred)\n",
    "            print(pred)\n",
    "\n",
    "        # Create geo_pred_df with pred column\n",
    "        geo_pred_df = test_gdf[id_cols].copy()\n",
    "        geo_pred_df['PredictedDailyNewCases'] = geo_preds\n",
    "        geo_pred_dfs.append(geo_pred_df)\n",
    "\n",
    "    # Combine all predictions into a single dataframe\n",
    "    pred_df = pd.concat(geo_pred_dfs)\n",
    "    \n",
    "    # Drop GeoID column to match expected output format\n",
    "    pred_df = pred_df.drop(columns=['GeoID'])\n",
    "    pred_df\n",
    "    \n",
    "    # Write predictions to csv\n",
    "    # Save to expected file name\n",
    "    output_file_name = start_date + \"_\" + end_date + \".csv\"\n",
    "    pred_df.to_csv(output_file_name, index=None)\n",
    "    print(f\"Predictions saved to {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2020-08-01\"\n",
    "end_date = \"2020-08-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predict(start_date, end_date, path_to_ips_file=\"../validation/data/2020-08-01_2020-08-31_ip.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that predictions are written correctly\n",
    "!head 2020-08-01_2020-08-31.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "from validation.validation import validate_submission\n",
    "\n",
    "errors = validate_submission(start_date, end_date, \"2020-08-01_2020-08-31.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "This is how the predictor is going to be called during the competition.  \n",
    "!!! PLEASE DO NOT CHANGE THE API !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2020-08-01\"\n",
    "end_date = \"2020-08-04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py -s {start_date} -e {end_date} -ip ../../validation/data/2020-08-01_2020-08-04_ip.csv -o predictions/2020-08-01_2020-08-04.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "from validation.validation import validate_submission\n",
    "\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-08-04\", \"predictions/2020-08-01_2020-08-04.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python predict.py -s 2020-08-01 -e 2020-08-31 -ip ../../validation/data/2020-08-01_2020-08-31_ip.csv -o predictions/2020-08-01_2020-08-31.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-08-31\", \"predictions/2020-08-01_2020-08-31.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!python predict.py -s 2020-08-01 -e 2020-09-30 -ip ../../validation/data/2020-08-01_2020-09-30_ip.csv -o predictions/2020-08-01_2020-09-30.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the pediction file is valid\n",
    "errors = validate_submission(\"2020-08-01\", \"2020-09-30\", \"predictions/2020-08-01_2020-09-30.csv\")\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
